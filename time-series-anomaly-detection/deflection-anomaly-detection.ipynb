{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7120713,"sourceType":"datasetVersion","datasetId":4107088}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T09:35:12.651588Z","iopub.execute_input":"2023-12-05T09:35:12.652178Z","iopub.status.idle":"2023-12-05T09:35:12.992832Z","shell.execute_reply.started":"2023-12-05T09:35:12.652143Z","shell.execute_reply":"2023-12-05T09:35:12.991944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install git+https://github.com/sintel-dev/Orion.git","metadata":{"execution":{"iopub.status.busy":"2023-12-05T09:35:13.869163Z","iopub.execute_input":"2023-12-05T09:35:13.870187Z","iopub.status.idle":"2023-12-05T09:36:04.429511Z","shell.execute_reply.started":"2023-12-05T09:35:13.870151Z","shell.execute_reply":"2023-12-05T09:36:04.428161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#查看orion-ml版本\n! pip freeze | grep orion-ml","metadata":{"execution":{"iopub.status.busy":"2023-12-05T09:36:08.807382Z","iopub.execute_input":"2023-12-05T09:36:08.808032Z","iopub.status.idle":"2023-12-05T09:36:11.391391Z","shell.execute_reply.started":"2023-12-05T09:36:08.808001Z","shell.execute_reply":"2023-12-05T09:36:11.390329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\nrm -rf Orion\nrm -rf images\n\ngit clone https://username:password@github.com/signals-dev/Orion.git\nmv Orion/tutorials/tulog/* .\nexit","metadata":{"execution":{"iopub.status.busy":"2023-12-05T09:36:12.242269Z","iopub.execute_input":"2023-12-05T09:36:12.242652Z","iopub.status.idle":"2023-12-05T09:36:14.805020Z","shell.execute_reply.started":"2023-12-05T09:36:12.242623Z","shell.execute_reply":"2023-12-05T09:36:14.804064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# in colab, primitives and pipelines don't show by default\n# we have to add them manually.\n\nfrom mlblocks import add_pipelines_path, add_primitives_path\n\nadd_primitives_path(\"/opt/conda/lib/python3.10/site-packages/mlstars/primitives/\")\nadd_primitives_path(\"/opt/conda/lib/python3.10/site-packages/orion/primitives/jsons/\")\nadd_pipelines_path(\"/opt/conda/lib/python3.10/site-packages/orion/pipelines/verified/tadgan/\")\nadd_pipelines_path(\"/opt/conda/lib/python3.10/site-packages/orion/pipelines/verified/dense_autoencoder/\")\nadd_pipelines_path(\"/opt/conda/lib/python3.10/site-packages/orion/pipelines/verified/vae/\")","metadata":{"execution":{"iopub.status.busy":"2023-12-05T09:36:16.663342Z","iopub.execute_input":"2023-12-05T09:36:16.663671Z","iopub.status.idle":"2023-12-05T09:36:16.707820Z","shell.execute_reply.started":"2023-12-05T09:36:16.663643Z","shell.execute_reply":"2023-12-05T09:36:16.707132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general imports\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# from utils import plot, plot_ts, plot_rws, plot_error, unroll_ts","metadata":{"execution":{"iopub.status.busy":"2023-12-05T09:36:34.031492Z","iopub.execute_input":"2023-12-05T09:36:34.031914Z","iopub.status.idle":"2023-12-05T09:36:34.037158Z","shell.execute_reply.started":"2023-12-05T09:36:34.031881Z","shell.execute_reply":"2023-12-05T09:36:34.036236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 读取CSV文件并加载数据\ndata = pd.read_csv('/kaggle/input/anomaly-dataset/deflection_with_label.csv')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T09:36:37.440347Z","iopub.execute_input":"2023-12-05T09:36:37.441281Z","iopub.status.idle":"2023-12-05T09:36:37.474769Z","shell.execute_reply.started":"2023-12-05T09:36:37.441241Z","shell.execute_reply":"2023-12-05T09:36:37.473904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 将索引转换为datetime类型\ndata['date'] = pd.to_datetime(data['date'])\n\n# 将datetime类型列转换为以分钟为单位的Unix时间戳\ndata['date'] = (data['date'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(seconds=1)\n\n# 修改列名\ndata = data.rename(columns={'date': 'timestamp'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 获取异常标签区间\nresult = []\n\nstart = None\nfor idx, value in data['deflection_warning'].items():\n    if value == 1:\n        if start is None:\n            start = data['timestamp'][idx]\n    else:\n        if start is not None:\n            end = data['timestamp'][idx] - 900\n            result.append((start, end))\n            start = None\n\n# 处理最后一个区间\nif start is not None:\n    end = df.index[-1]\n    result.append((start, end))\n    \n# 定义DataFrame类型变量\nanomaly_df = pd.DataFrame(columns=['start', 'end'])\nfor r in result:\n    # 插入一行数据\n    new_row = {'start': r[0], 'end': r[1]}\n    anomaly_df = pd.concat([anomaly_df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\nanomaly_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.drop(\"deflection_warning\", axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pandas.plotting import register_matplotlib_converters\ndef convert_date(timelist):\n    converted = list()\n    for x in timelist:\n        converted.append(datetime.fromtimestamp(x))\n    return converted\ndef convert_date_single(x):\n    return datetime.fromtimestamp(x)\ndef plot_custom(dfs, anomalies=[]):\n    if isinstance(dfs, pd.DataFrame):\n        dfs = [dfs]\n\n    if not isinstance(anomalies, list):\n        anomalies = [anomalies]\n\n    df = dfs[0]\n    time = convert_date(df['timestamp'])\n\n    fig = plt.figure(figsize=(30, 6))\n    ax = fig.add_subplot(111)\n\n    for df in dfs:\n        for i in range(1,19):\n            plt.plot(time, df.iloc[:, i]) #0-timestamps\n\n    colors = ['red'] + ['green'] * (len(anomalies) - 1)\n    for i, anomaly in enumerate(anomalies):\n        if not isinstance(anomaly, list):\n            anomaly = list(anomaly[['start', 'end']].itertuples(index=False))\n\n        for _, anom in enumerate(anomaly):\n            t1 = convert_date_single(anom[0])\n            t2 = convert_date_single(anom[1])\n            plt.axvspan(t1, t2, color=colors[i], alpha=0.2)\n    plt.ylabel('value', size=30)\n    plt.xlabel('Time', size=30)\n    plt.xticks(size=26)\n    plt.yticks(size=26)\n    plt.xlim([time[0], time[-1]])\n    plt.show()\nplot_custom(data,anomaly_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练速度快\nfrom orion import Orion\n\nanomalies1=pd.DataFrame()\nfor i in range(1,19):\n  hyperparameters = {\n      \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n          'interval': 900\n      },\n      \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n              \"target_column\":0,\n              \"window_size\": 1000\n          },\n      \"keras.Sequential.DenseSeq2Seq#1\": {\n              \"epochs\": 20,\n              \"verbose\": True,\n              \"window_size\": 1000,\n              \"input_shape\": [1000, 1],\n              \"target_shape\": [1000,1]\n          },\n      \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n              \"window_size_portion\": 0.33,\n              \"window_step_size_portion\": 0.1,\n              \"fixed_threshold\": False\n          }\n  }\n  dense_autoencoder = Orion(\n      pipeline='/kaggle/working/Orion/orion/pipelines/verified/dense_autoencoder/dense_autoencoder.json',\n      hyperparameters=hyperparameters\n  )\n  anomalies1 = pd.concat([anomalies1, dense_autoencoder.fit_detect(pd.concat([data.iloc[:,0],data.iloc[:,i]], axis=1))])\n  import pickle\n  # save model\n  with open(\"/kaggle/working/dense_autoencoder_\"+data.columns[i]+\".pkl\", \"wb\") as pickle_file:\n      pickle.dump(dense_autoencoder, pickle_file)\nanomalies1.to_csv(\"/kaggle/working/deflection_dense_autoencoder.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_custom(data, [anomalies1, anomaly_df])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练速度快\nfrom orion import Orion\n\nanomalies2=pd.DataFrame()\nfor i in range(1,19):\n  hyperparameters = {\n    \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n            \"time_column\": \"timestamp\",\n            \"interval\": 900,\n            \"method\": \"mean\"\n        },\n    'sklearn.preprocessing.MinMaxScaler#1': {\n        'feature_range': (-1, 1)\n    },\n    \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n            \"target_column\": 0,\n            \"window_size\": 1000,\n            \"target_size\": 1,\n        },\n    \"orion.primitives.aer.AER#1\": {\n#             \"epochs\": 1,\n            \"batch_size\": 512,\n        },\n    \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n            \"window_size_portion\": 0.33,\n            \"window_step_size_portion\": 0.1,\n            \"fixed_threshold\": False\n        }\n  }\n  aer = Orion(\n      pipeline='/kaggle/working/Orion/orion/pipelines/verified/aer/aer.json',\n      hyperparameters=hyperparameters\n  )\n  anomalies2 = pd.concat([anomalies2, aer.fit_detect(pd.concat([data.iloc[:,0],data.iloc[:,i]], axis=1))])\n  import pickle\n  # save model\n  with open(\"/kaggle/working/aer_\"+data.columns[i]+\".pkl\", \"wb\") as pickle_file:\n      pickle.dump(aer, pickle_file)\nanomalies2.to_csv(\"/kaggle/working/deflection_aer.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_custom(data, [anomalies2, anomaly_df])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomalies3=pd.DataFrame()\nfor i in range(1,19):\n  hyperparameters = {\n    \"mlstars.custom.timeseries_preprocessing.time_segments_aggregate#1\": {\n            \"interval\": 900,\n        },\n    \"mlstars.custom.timeseries_preprocessing.rolling_window_sequences#1\": {\n            \"target_column\": 0,\n            \"window_size\": 1000,\n            \"target_size\": 1,\n        },\n    \"orion.primitives.vae.VAE#1\": {\n            \"verbose\": True,\n            \"batch_size\": 512,\n        },\n    \"orion.primitives.timeseries_anomalies.find_anomalies#1\": {\n            \"window_size_portion\": 0.33,\n            \"window_step_size_portion\": 0.1,\n            \"fixed_threshold\": False\n        }\n  }\n  vae = Orion(\n      pipeline='/kaggle/working/Orion/orion/pipelines/verified/vae/vae.json',\n      hyperparameters=hyperparameters\n  )\n  anomalies3 = pd.concat([anomalies3, vae.fit_detect(pd.concat([data.iloc[:,0],data.iloc[:,i]], axis=1))])\n  import pickle\n  # save model\n  with open(\"/kaggle/working/vae_\"+data.columns[i]+\".pkl\", \"wb\") as pickle_file:\n      pickle.dump(vae, pickle_file)\nanomalies3.to_csv(\"/kaggle/working/deflection_vae.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_custom(data, [anomalies3, anomaly_df])","metadata":{},"execution_count":null,"outputs":[]}]}